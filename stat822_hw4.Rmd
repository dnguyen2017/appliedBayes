---
title: "STAT 822 - HW 4"
author: "David Nguyen"
date: "February 26, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path="figures/")
```

```{r}
# use inverse cdf method to draw from N(0,1) using draws from U(0,1)
sampler <- function(n){
  qnorm(runif(n))
}
# generate 10000 random observations
samples <- sampler(10000)
```

```{r}
# compare sampler() to normal pdf
hist(samples, prob = TRUE, xlim = c(-4, 4), ylim = c(0, 0.4),
     main = "Comparison of simulated and true density",
     xlab = "Value of random variable")
curve(dnorm(x), -4, 4, col = "red", add = TRUE)
```

```{r message = FALSE}
library(dplyr) # contains case_when
# Function to sample from triangular distn
sampler_triangle <- function(n) {
  tibble(u = runif(n),
         x = case_when(u < 0 ~ NA_real_,
          ((0 <= u) & (u <= 0.25)) ~ sqrt(u/4),
           ((0.25 < u) & (u < 1)) ~ 1/2 * (2 - sqrt(3) * sqrt(1 - u)),
          1 <= u ~ NA_real_))  
}
# pdf of triangular distn
triangle <- function(x) {
  case_when(((0 <= x) & (x < 0.25)) ~ 8*x,
            ((0.25 <= x) & (x <= 1)) ~ (8/3) âˆ’ (8/3) * x,
            x > 1 ~ 0)
}
# generate samples
samples_triangle <- sampler_triangle(10000)
```

```{r}
# compare sampler_triangle() to true pdf
hist(samples_triangle$x, prob = TRUE,
     main = "Comparison of simulated and true density",
     xlab = "Value of random variable")
curve(triangle(x), 0, 1.1, col = "red", add = TRUE)
```

# question 3

```{r}
pDpos <- 0.001
pDneg <- 1 - pDpos
sensitivity <- 0.99
specificity <- 0.95
pfalseneg <- 1 - sensitivity
pfalsepos <- 1 - specificity

lik <- pDpos * sensitivity
marginal <- pDpos * sensitivity + pDneg * pfalsepos
(posterior <- lik/marginal)
```

# question 4

```{r}
lik2 <- posterior * pfalseneg
marginal2 <- posterior * pfalseneg + (1 - posterior) * specificity
(posterior2 <- lik2/marginal2)
```

# question 5

```{r}
lik3 <- pDpos * pfalseneg
marginal3 <- pDpos * pfalseneg + pDneg * specificity
(posterior3 <- lik3/marginal3)

lik4 <- posterior3 * sensitivity
marginal4 <- posterior3 * sensitivity + (1-posterior3) * pfalsepos
lik4/marginal4
```

```{r}
# both tests in at the same time
lik5 <- pDpos * pfalseneg * sensitivity
marginal5 <- pDpos * pfalseneg * sensitivity + pDneg * specificity * pfalsepos
posterior5 <- lik5/marginal5
```

The order that the tests are observed doesn't matter, which is demonstrated by the RHS of the code chunk above (since tests are independent, $P(T^+,T^-|D^+) = P(T^-,T^+|D^+) = P(T^-,D^+)P(T^+,D^+) = P(T^+,D^+)P(T^-,D^+)$)

# question 6

```{r}
library(LearnBayes)
# given
p <- seq(0,1, length = 9)
prior <- c(0.001, 0.001, 0.950, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008)
data <- c(6, 4) # (success, failure)
# calculate posterior
post6 <- pdisc(p, prior, data)
```

```{r}
plot(x = p, y = prior, ylab = "Probability", main = "Probability that Bob has ESP", xlab = "Proportion (p)")
points(x = p, y = post6, col = "red", pch = 2)
legend(x = 0.8, y = 0.8, legend = c("prior", "posterior"), col = c("black", "red"), pch = c(1,2))
```

