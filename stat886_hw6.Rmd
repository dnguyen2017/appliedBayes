---
title: "HW 6"
author: "David Nguyen"
date: "March 14, 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 10.2
Number of simulation draws: suppose you are interested in inference for the parameter $\theta_1$ in a multivariate posterior distribution, $p(\theta | y)$. You draw 100 independent values $\theta$ from the posterior distribution of $\theta$ and find that the posterior density for $\theta_1$ is approximately normal with mean of about 8 and standard deviation of about 4.

a.) Using the average of the 100 draws of $\theta_1$ to estimate the posterior mean, $E(\theta_1|y)$, what is the approximate standard deviation due to simulation variability?

**Solution:**

The approximate standard deviation due to simulation variability is: 

${\theta} \sqrt{1 + 1/S} = \sqrt{1 + 1/100} =$ `r sqrt(1 + 1/100)`

This implies that very little of the uncertainty in the posterior variance of $\theta_1$ is contributed by Monte Carlo error.

b.) About how many simulation draws would you need to reduce the simulation standard deviation of the posterior mean to 0.1 (thus justifying the presentation of results to one decimal place).

**Solution:**

$s_{\theta} / \sqrt{S} = 4 / \sqrt{100} = 0.4$

To reduce this quantity to 0.1 requires we take:

\begin{align*}
4 / \sqrt{S} &= 0.1 \\
\implies S &= 40^2 \\
 &= 1600 \text{ samples}
\end{align*}

c.) A more usual summary of the posterior distribtuion of $\theta_1$ is a 95% central posterior interval. Based on the data from 100 draws, what are the approximate simulation standard eviations of the estimated 2.5% and 97.5% quantiles of the posterior distribution? (Recall that the posterior density is approximately normal).

**Solution:**

Note that posterior probabilites are estimated to a standard deviation of $\sqrt{p(1-p)/S}$.

So, the standard deviations associated with both the 2.5% and 97.5% probabilities is 

$\sqrt{0.025 \times 0.975 / 100} =$ `r sqrt(0.025 * (1-0.025)/100)`.

# 10.3
Posterior computations for a binomial model: suppose $y_1 \sim Bin(n_1, p_1)$ is the number of patients successfully treated under an experimental new drug, and $y_2 \sim Bin(n_2, p_2)$ is the number of successfully treated patients under the standard treatment. Assume that $y_1$ and $y_2$ are independent and assume beta prior densities for the two probabilities of success. Let $n_1 = 10, y_1 = 6$ and $n_2 = 20, y_2 = 10$. Repeat the following for several different beta prior densities.

a.) Use simulation to find a 95% posterior interval for $p_1 - p_2$ and the posterior probability that $p_1 > p_2$.

**Solution:**

Recall that the beta distribution is conjugate for the binomial probability of success and that the posterior is $p_i|\boldsymbol{y} \sim \text{beta}(\alpha + \sum y_i, n - \sum y_i + \beta)$.

I will consider two priors: 

* Flat: $p_1(p_i) \sim \text{beta}(1,1)$
* Jeffrey's: $p_2(p_i) \sim \text{beta}(0.5,0.5)$

```{r out.width="50%", fig.show="hold"}
set.seed(123)
# data
n1 <- 10; y1 <- 6; n2 <- 20; y2 <- 10; 
# same priors for treatments 1 and 2. for these priors, alpha = beta
ab_flat <- 1
ab_jeff <- 0.5

# take 1000 samples from p1 - p2
nsamples <- 1000
# flat prior
post_p1_flat <- rbeta(nsamples, shape1 = ab_flat + y1, shape2 = n1 - y1 + ab_flat)
post_p2_flat <- rbeta(nsamples, shape1 = ab_flat + y2, shape2 = n2 - y2 + ab_flat)
post_diff_flat <- post_p1_flat - post_p2_flat

# jeffrey's prior
post_p1_jeff <- rbeta(nsamples, shape1 = ab_jeff + y1, shape2 = n1 - y1 + ab_jeff)
post_p2_jeff <- rbeta(nsamples, shape1 = ab_jeff + y2, shape2 = n2 - y2 + ab_jeff)
post_diff_jeff <- post_p1_jeff - post_p2_jeff

# plot histograms of posterior
xlim_diff <- c(-0.7, 0.7)
hist(post_diff_flat, prob = TRUE, xlab = expression(p[1] - p[2]), 
     main = "Beta(1,1) prior", xlim = xlim_diff)
hist(post_diff_jeff, prob = TRUE, xlab = expression(p[1] - p[2]), 
     main = "Beta(0.5,0.5) prior", xlim = xlim_diff)

# get 95% posterior CrI
CI_diff_flat <- round(quantile(post_diff_flat, c(0.025, 0.975)), 2)
CI_diff_jeff <- round(quantile(post_diff_jeff, c(0.025, 0.975)), 2)
```

The 95% credible intervals are 

* Flat prior, $\text{beta}(1,1)$: `r CI_diff_flat[1]` $< p_1 -p_2 <$  `r CI_diff_flat[2]`
* Jeffrey's prior, $\text{beta}(0.5,0.5)$: `r CI_diff_jeff[1]` $< p_1 -p_2 <$  `r CI_diff_jeff[2]`

b. Numerically integrate to estimate the posterior probability that $p_1 > p_2$.

```{r}
# get from posterior simulations
length(post_diff_flat[post_diff_flat > 0]) / nsamples
length(post_diff_jeff[post_diff_jeff > 0]) / nsamples
```

```{r}
# get by numerical integration

```


# 3

Sample from a mixture of two beta distributions: $0.7 \times \text{beta}(4,2) + 0.3 \times \text{beta}(1.5,3)$

```{r}
beta3 <- function(x) 0.7*dbeta(x, 4, 2) + 0.3*dbeta(x, 1.5, 3)
```

a. Devise and implement a rejection sampler. Generate 1000 samples, plot a histogram, and plot the true density.

```{r}
set.seed(123)
# use U(0,1) as a proposal distn g(x)
# find scalar m, to make m * g(x) > f(x)
# max(beta3(seq(0, 1, length.out = 1000))) # 1.594115

# reject-accept sampling
beta3_reject <- function(n, m = 1.6) {
  y <- runif(n) # proposal
  ratio <- beta3(y)/m # f(y) / m*g(y); note, g(y) = 1 for g(y) ~ Uniform(0,1)
  u <- runif(n) # to compare with ratio
  accept <- u < ratio
  samples <- y[accept]
  return(list(samples = samples, p.accept = length(samples)/n))
}

# generate 1000 samples
samples_rej <- beta3_reject(2000)

# plot histogram and true density
hist(samples_rej$samples, prob = TRUE, 
     main = "Rejection sampling for beta mixture",
     ylab = "Density f(x)",
     xlab = "x")
curve(beta3(x), 0, 1, add = TRUE)
text(0.1, 1.4, labels = paste("P(accept) = ", samples_rej$p.accept))
```

Since the acceptance rate was around 0.6, I drew 2000 samples from the proposal distribution to get at least 1000 draws from the target distribution.

b. Apply a Metropolis-Hastings algorithm to get a sample size of 1000 after a burn-in sample of 500.  Choose your own Beta(a, b) candidate distribution. What is the acceptance rate?

```{r}
b <- 5
curve(dbeta(x, 1.5*b,b))
curve(beta3(x), add = TRUE, col = "red")

```


c. Devise a random walk Metropolis algorithm to get a sample of size 1000 after a burn-in sample of 500.  Use a normal proposal distribution but be careful becausethe target distribution only has support on [0,1]. What is the standard deviation you chose for the random walk? What is the acceptance rate?