---
title: "stat 822 hw 6"
author: "David Nguyen"
date: "March 26, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# 1. 

estimating a normal mean with a discrete prior

Want to estimate the average total snowfall per year $\mu$ is discrete with probilities:

```{r}
# discrete prior for mean snowfall
mu <- seq(20, 70, by = 10)
prior <- c(0.1, 0.15, 0.25, 0.25, 0.15, 0.1)
rbind(mu, prior) %>% knitr::kable()

```

```{r}
# observed data
y <- c(38.6, 42.4, 57.5, 40.5, 51.7, 67.1, 33.4, 60.9, 64.1, 40.1, 40.7, 6.4)
ybar <- mean(y)
```

a.

The likelihood function is given by

```{r}
n <- 12
sigma <- 10
(like <- exp(-(n/(2*sigma^2)) * (mu - ybar)^2))
rbind(mu, prior, like) %>% knitr::kable()
```

```{r}
# calculate posterior
(posterior <- prior * like / sum(prior * like))
# check that posterior is proper
sum(posterior) == 1
```

```{r}
tibble(mu, prior, like, posterior) %>% knitr::kable()
```

# 2

```{r}
alpha0 <- 16
beta0 <- 15174
yA <- 1; eA <- 66
yB <- 4; eB <- 1767
```

```{r}
# hospital A posterior predictive density
n.sims <- 100000
# prior <- rgamma(n.sims, shape = alpha0, rate = beta0)
posterior_A <- rgamma(n.sims, shape = alpha0 + yA, rate = beta0 + eA)
y_post_pred_A <- rpois(n = n.sims, lambda = eA*posterior_A)
hist(posterior_A)
# hist(prior)
hist(y_post_pred_A, prob = TRUE)
# hist(y_prior_pred, prob = TRUE)
unique(y_post_pred_A)

# want to know prob of getting y_new = 0, 1, ..., 10
y_new <- 0:10
prob_y_new_A <- vector("numeric", length = length(y_new))

for (y in seq_along(y_new)) {
  prob_y_new_A[y] <- mean(y_post_pred_A == y_new[y])
}

prob_y_new_A
```

```{r}
prior <- rgamma(n.sims, shape = alpha0, rate = beta0)
posterior_B <- rgamma(n.sims, shape = alpha0 + yB, rate = beta0 + eB)
y_post_pred_B <- rpois(n = n.sims, lambda = eB*posterior_B)
y_prior_pred_B <- rpois(n = n.sims, lambda = eB*prior)
hist(posterior_B, prob = TRUE)
hist(prior, prob = TRUE)
hist(y_post_pred_B, prob = TRUE)
hist(y_prior_pred_B, prob = TRUE)
unique(y_post_pred_B)
unique(y_prior_pred_B)

# get prob(y_new_B | y, e)
prob_y_new_B <- vector("numeric", length = length(y_new))

for (y in seq_along(y_new)) {
  prob_y_new_B[y] <- mean(y_post_pred_B == y_new[y])
}

prob_y_new_B
```

```{r}
clabels <- paste("P(Y = ", 0:10, ")", sep = "")
rbind("hospital A" = prob_y_new_A, "hospital B" = prob_y_new_B) %>% knitr::kable(col.names = clabels)
tibble("P(Y=y)" = 0:10, "hospital A" = prob_y_new_A, "hospital B" = prob_y_new_B) %>% knitr::kable()
```


# 3

a. 

$$\ell(\beta;y_i) = y_i (\beta_0 + \beta_1 x_i) + \log(1+e^{\beta_0+\beta_1 x_i})$$

b.

```{r}
face <- read.table("facerecognition.dat", header = TRUE)
head(face) 
```

```{r}
fit.glm <- glm(match ~ eyediff, data = face, family = binomial(link = "logit"))
coef(fit.glm)
```

```{r error=TRUE}
# data
n.obs <- nrow(face)
y <- face$match
x <- face$eyediff
ybar <- mean(y)
nybar <- n.obs * ybar
xbar <- mean(x)
nxbar <- n.obs * xbar

# compute score at current beta
get_score <- function(beta) {
  exp.eta <- exp(beta[1,1] + beta[2,1] * x)
  inv.logit.eta <- exp.eta / (1 + exp.eta)
  db0 <- sum(y + inv.logit.eta)
  db1 <- sum(y*x + x * inv.logit.eta)
  return(matrix(c(db0, db1), nrow = 2))
}

# compute observed fisher information
get_fish <- function(beta) {
  exp.eta <- exp(beta[1,1] + beta[2,1] * x)
  denom <- (1 + exp.eta)^2
  h11 <- sum(exp.eta / denom) 
  h12 <- h21 <- sum(exp.eta * x/ denom)
  h22 <- sum(exp.eta * x * x / denom) 
  hess <- matrix(c(h11, h12,
                   h21, h22), 
                 byrow = TRUE, nrow = 2, ncol = 2)
  obs.fisher <- - solve(hess)
  return(obs.fisher)
}
  
update_beta <- function(beta, score, fish) {
  new_beta <- beta + fish %*% score
  return(new_beta)
}

# define log likelihood function for model
# for second convergence criteria
loglik <- function(pars) {
  lin.pred <- pars[1] + pars[2] * x
  pi.hat <- exp(lin.pred) / (1 + exp(lin.pred))
  nll <- sum(dbinom(x = y, size = 1, prob = pi.hat, log = TRUE))
  return(nll)
}

fisher_scoring <- function(guess, maxit, tol = rep(1e-4,3)) {
  # initialization
  beta_matrix <- matrix(0, nrow = length(guess), ncol = maxit + 1)
  score_list <- fisher_list <- vector("list", length = maxit)
  conv_beta <- conv_lik <- conv_norm <- vector("logical", length = maxit)
  conv_fit <- FALSE
  beta_matrix[,1] <- guess # set beta0
  # fisher scoring algorithm
  for (iter in seq_along(score_list)) {
    print(iter)
    score_list[[iter]] <- get_score(beta_matrix[,iter, drop = FALSE])
    fisher_list[[iter]] <- get_fish(beta_matrix[,iter, drop = FALSE])
    beta_matrix[,iter + 1] <- update_beta(beta_matrix[,iter, drop = FALSE], score_list[[iter]], fisher_list[[iter]])
    
    # check convergence
    conv_beta[iter] <- sqrt(sum(score_list[[iter]]^2)) <= tol[1]
    conv_lik[iter] <- abs(loglik(beta_matrix[,iter+1]) - loglik(beta_matrix[,iter])) <= tol[2]
    conv_norm[iter] <- sqrt(sum((beta_matrix[,iter + 1] - beta_matrix[, iter])^2)) <= tol[3]
    
    print(beta_matrix[,iter+1])
    # if convergence criteria met, break loop
    if (conv_beta[iter] & conv_lik[iter] & conv_norm[iter]) {
      conv_fit <- TRUE
      break
    }
  }
  
  # print final estimate of beta (may not be MLE)
  conv_message <- paste("model converged? ", conv_fit)
  print(conv_message)
  print(beta_matrix[,maxit+1])
  return(list(beta_matrix, score_list, fisher_list))
}

try_beta = matrix(c(1.5, -10), nrow = 2)
fisher_scoring(guess = try_beta, maxit = 25)
```

Instead of the Fisher-scoring algorithm, we can just minimize the negative log-likelihood of the model instead.

```{r}
# define negative log likelihood function for model
# for second convergence criteria
negloglik <- function(pars) {
  lin.pred <- pars[1] + pars[2] * x
  pi.hat <- exp(lin.pred) / (1 + exp(lin.pred))
  nll <- -sum(dbinom(x = y, size = 1, prob = pi.hat, log = TRUE))
  return(nll)
}
# initial parameter guess
# choose "bad" guess on purpose
par_guess <- c(0,0)

# get MLE
fit <- optim(par = par_guess, fn = negloglik, method = "Nelder-Mead")
fit$par 

# print comparison of glm() vs. my solution
tibble(parameter = c("Intercept", "eyediff"),
       glm = coef(fit.glm),
       optim = fit$par,
       difference = glm - optim) %>%
  knitr::kable()
```

